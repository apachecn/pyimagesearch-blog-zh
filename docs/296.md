# 梯度下降算法和变体

> 原文：<https://pyimagesearch.com/2021/05/05/gradient-descent-algorithms-and-variations/>

在本教程中，您将学习:

*   什么是梯度下降
*   梯度下降如何让我们训练神经网络
*   梯度下降的变化，包括随机梯度下降(SGD)
*   如何利用动量和内斯特罗夫加速度提高 SGD

**要了解梯度下降及其变化，** ***只需继续阅读。***

## **梯度下降算法和变体**

当涉及到训练神经网络时，梯度下降不仅仅是主力——它是耕地的犁和控制犁走向的农民。

梯度下降和优化器有大量的变化，从普通梯度下降、小批量梯度下降、随机梯度下降(SGD)和小批量 SGD，仅举几例。

此外，考虑到对 SGD 的改进，设计了全新的模型优化器，包括 Adam、Adadelta、RMSprop 等。

今天我们将回顾梯度下降的基本原理，并主要关注 SGD，包括对 SGD 的两项改进，**动量**和**内斯特罗夫加速。**

### **什么是梯度下降？**

梯度下降是一种一阶优化算法。梯度下降的目标是找到一个可微函数的局部最小值。

**我们迭代执行梯度下降*****:***

1.  我们从成本/损失函数(即负责计算我们想要最小化的值的函数)开始
2.  然后我们计算损失的梯度
3.  最后，我们在与梯度方向相反的方向迈出一步(因为这将把我们带到局部最小值的路径上)

下图简要总结了梯度下降:

但是这如何应用于神经网络和深度学习呢？

让我们在下一部分解决这个问题。

### **梯度下降如何为神经网络和深度学习提供动力？**

神经网络由一个或多个隐藏层组成。每一层都由一组参数组成。我们的目标是优化这些参数，使我们的损失最小化。

典型的损失函数包括二元交叉熵(两类分类)、分类交叉熵(多类分类)、均方误差(回归)等。

损失函数有*多种*类型，每一种都用于特定的角色。不要太纠结于正在使用哪个损失函数的*，而是这样想:*

1.  我们用一组随机的权重初始化我们的神经网络
2.  我们要求神经网络对我们训练集中的数据点进行预测
3.  我们计算预测，然后计算损失/成本函数，它告诉我们在做出正确预测时做得有多好/多差
4.  我们计算损失的梯度
5.  然后我们*稍微调整一下神经网络的参数，这样我们的预测会更好*

我们一遍又一遍地这样做，直到我们的模型被称为“收敛”并能够做出可靠、准确的预测。

梯度下降算法有很多种，但我们今天将重点介绍的是:

1.  香草梯度下降
2.  随机梯度下降
3.  小批量 SGD
4.  带动量的 SGD
5.  带内斯特罗夫加速的 SGD

### **香草梯度下降**

考虑一个由 *N=10，000* 幅图像组成的图像数据集。我们的目标是训练一个神经网络，将这 10，000 张图像中的每一张分类成总共 *T=10* 个类别。

为了在这个数据集上训练神经网络，我们将利用梯度下降。

梯度下降的最基本形式，我喜欢称之为**香草梯度下降**，我们每次更新只更新网络的权重*一次*。

这意味着:

1.  我们通过我们的网络运行所有 10，000 张图片
2.  我们计算损耗和梯度
3.  我们更新网络的参数

在普通梯度下降中，我们每次迭代只更新网络的权重*一次*，这意味着每次执行权重更新时，网络都会看到整个*数据集。*

实际上，这没什么用。

**如果训练样本的数量很大，那么香草梯度下降将花费** ***很长时间来收敛*** **，因为权重更新在每个数据周期仅发生** ***一次*** **。**

此外，你的数据集变得越大，你的梯度就变得越细微，如果**你每个时期只更新一次权重，那么你将花费大部分时间来计算预测，而不是花费太多时间来学习***(这是优化问题的目标，对吗？)*

 *幸运的是，还有其他梯度下降的变体可以解决这个问题。

### **随机梯度下降(SGD)**

与每个历元仅进行一次权重更新的普通梯度下降不同，随机梯度下降(SGD)进行*多次*权重更新。

SGD 的原始公式将在每个历元进行 *N* 权重更新，其中 *N* 等于数据集中数据点的总数。因此，使用我们上面的例子，如果我们有 *N=10，000 个*图像，那么我们每个时期将有 10，000 个权重更新。

SGD 算法变成:

*   直到收敛:
    *   从数据集中随机选择一个数据点
    *   对此做一个预测
    *   计算损耗和梯度
    *   更新网络的参数

**SGD 趋向于收敛** ***快得多*** **因为它能够在每次权重更新后开始自我改进。**

也就是说，在每个时期执行 *N* 权重更新(其中 *N* 等于我们数据集中的数据点总数)在计算上也有点浪费——我们现在已经转到了钟摆的另一边。

相反，我们需要一个介于两者之间的中间值。

### **小批量新币**

虽然 SGD 可以更快地收敛于大型数据集，但我们实际上遇到了另一个问题——我们无法利用我们的矢量化库来实现超快训练(同样，因为我们一次只能通过网络传递一个数据点)。

有一种称为**小批量 SGD** 的 SGD 变体解决了这个问题。**当你听到人们谈论新币时，他们几乎总是指的是** ***迷你批量新币。***

小批量 SGD 引入了批量大小的概念。现在，给定一个大小为 *N* 的数据集，总共会有 *N / S* 次网络更新。

我们可以将小批量 SGD 算法总结为:

*   随机打乱输入数据
*   直到收敛:
    *   选择大小为 *S* 的下一批数据
    *   对子集进行预测
    *   计算小批量的损耗和平均梯度
    *   更新网络的参数

如果您直接可视化每个小批量，那么您将看到一个非常嘈杂的图，如下图所示:

但是当你平均所有小批量的损失时，曲线实际上是相当稳定的:

***注意:*** *根据你使用的深度学习库，你可能会看到这两种类型的图。*

当你听到深度学习实践者谈论 SGD 时，他们*更有可能*谈论小批量 SGD。

### **带动量的新币**

SGD 的一个问题是，在亏损领域的某个维度比其他维度明显更陡(你会在 local optima 附近看到这种情况)。

当这种情况发生时，似乎 SGD 只是振荡谷，而不是下降到较低损耗和理想情况下较低精度的区域(参见 [Sebastian Ruder 的精彩文章](https://ruder.io/optimizing-gradient-descent/)了解关于这一现象的更多详细信息)。

通过施加动量(**图 6** )，我们在一个方向上建立了一个蒸汽头，然后让重力使我们越滚越快地下山。

通常，在大多数 SGD 应用中，您会看到动量值为`0.9`。

习惯于在使用 SGD 时看到动量——它用于大多数应用 SGD 的神经网络实验中。

### **带内斯特罗夫加速度的 SGD**

动量的*问题*是，一旦你形成一股蒸汽，火车很容易失去控制，滚过我们当地的最小值，再次回到山上。

基本上，我们不应该*盲目地*跟随梯度的斜率。

内斯特罗夫加速说明了这一点，并帮助我们认识到亏损的局面何时开始再次上升。

几乎所有包含 SGD 实现的深度学习库*也*包括动量和内斯特罗夫加速项。

势头几乎总是一个好主意。内斯特罗夫加速在某些情况下有效，但在其他情况下无效。在训练神经网络时，您会希望将它们视为需要调整的超参数(即，为每个超参数选择值，运行实验，记录结果，更新参数，并重复进行，直到找到一组产生良好结果的超参数)。

[此外，我们有一整套关于超参数优化的教程，您可以在这里找到。](https://pyimagesearch.com/2021/05/17/introduction-to-hyperparameter-tuning-with-scikit-learn-and-python/)

## **总结**

在本教程中，您学习了梯度和下降及其变体，即随机梯度下降(SGD)。

SGD 是深度学习的主力。所有优化器，包括 Adam，Adadelta，RMSprop 等。这些优化器中的每一个都为 SGD 提供了调整和变化，理想地改善了收敛性，并使模型在训练过程中更加稳定。

我们将很快介绍这些更高级的优化器，但是目前，要理解 SGD 是所有这些优化器的基础。

我们可以通过引入动量项来进一步改进 SGD(几乎总是被推荐)。

有时，内斯特罗夫加速可以进一步提高 SGD(取决于您的具体项目)。

**要下载这篇文章的源代码(并在未来教程在 PyImageSearch 上发布时得到通知)，*只需在下面的表格中输入您的电子邮件地址！****