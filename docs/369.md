# 训练任何神经网络时的四个关键因素

> 原文：<https://pyimagesearch.com/2021/05/06/the-four-key-ingredients-when-training-any-neural-network/>

在训练神经网络时，您可能已经开始注意到我们的 Python 代码示例中的一个模式。在你自己的神经网络和深度学习算法中，有四个主要成分需要放在一起:一个*数据集*，一个*模型/架构*，一个*损失函数*，一个*优化方法*。我们将在下面逐一回顾这些成分。

### **数据集**

数据集是训练神经网络的第一要素——数据本身以及我们试图解决的问题定义了我们的最终目标。例如，我们是否正在使用神经网络进行回归分析，以预测特定郊区 20 年后的房价？我们的目标是执行无监督学习吗，比如降维？还是我们在尝试进行分类？

在本书的上下文中，我们严格地关注于*图像分类*；但是，数据集和您试图解决的问题的组合会影响您对损失函数、网络架构和用于训练模型的优化方法的选择。通常，我们在数据集中几乎没有选择的余地(除非你正在做一个业余爱好项目)——我们被给予一个数据集，这个数据集对我们项目的结果应该是什么有一些期望。然后由我们在数据集上训练一个机器学习模型，以便在给定的任务中表现良好。

### **损失函数**

给定我们的数据集和目标，我们需要定义一个与我们试图解决的问题一致的损失函数。在几乎所有使用深度学习的图像分类问题中，我们将使用交叉熵损失。对于 *> 2* 类，我们称之为*分类交叉熵*。对于两类问题，我们称损失*为二元交叉熵*。

### **模型/架构**

您的网络架构可以被认为是您必须做出的第一个实际“选择”。您的数据集很可能是为您选择的(或者至少您已经*决定*您想要使用给定的数据集)。如果你在进行分类，你很可能会使用交叉熵作为损失函数。

但是，您的网络架构可能会有很大的不同，尤其是当您选择使用哪种优化方法来训练网络时。在花时间研究您的数据集并查看:

1.  你有多少数据点？
2.  班级的数量。
3.  这些课有多相似/不相似。
4.  类内方差。

您应该开始对将要使用的网络体系结构有一种“感觉”。这需要实践，因为深度学习是部分科学，部分艺术。

请记住，随着您进行越来越多的实验，网络架构中的层数和节点数(以及任何类型的正则化)可能会发生变化。你收集的结果越多，你就越有能力做出明智的决定，决定下一步要尝试哪种技术。

### **优化方法**

最后一个要素是定义一个优化方法。 [*随机梯度下降*](https://pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/) 使用频率相当高。还存在其他优化方法，包括 RMSprop ( [Hinton、*用于机器学习的神经网络*](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) )、Adagrad ( [杜奇、哈赞和辛格，2011](http://dl.acm.org/citation.cfm?id=1953048.2021068) )、Adadelta ( [泽勒，2012](http://arxiv.org/abs/1212.5701) )和 Adam ( [金玛和巴，2014](http://arxiv.org/abs/1412.6980))；然而，这些是更高级的优化方法。

尽管有所有这些更新的优化方法，SGD 仍然是深度学习的主力——大多数神经网络都是通过 SGD 训练的，包括在 ImageNet 等具有挑战性的图像数据集上获得最先进精度的网络。

当训练深度学习网络时，特别是当你第一次开始学习时， *SGD 应该是你的首选优化器*。然后，您需要设置适当的学习速率和正则化强度、网络应接受训练的总时期数，以及是否应使用动量(如果是，使用哪个值)或内斯特罗夫加速度。花时间尽可能多地试验 SGD *和*变得适应调整 *参数*。

熟悉给定的优化算法类似于掌握如何驾驶汽车——你驾驶自己的车比别人的车好，因为你花了太多时间驾驶它；你了解你的车及其复杂性。通常，选择给定的优化器在数据集*而不是*上训练网络，因为优化器本身更好，但是因为*驱动者*(即深度学习实践者)*更熟悉优化器*，并且理解调整其各自参数背后的“艺术”。

请记住，即使对于高级深度学习用户来说，在小型/中型数据集上获得性能合理的神经网络也可能需要 10 到 100 次的实验——当您的网络一开始就表现不佳时，不要气馁。精通深度学习将需要你投入时间和许多实验——但是一旦你掌握了这些要素是如何组合在一起的，这将是值得的。