# è‡ªåŠ¨å¾®åˆ†ç¬¬ 2 éƒ¨åˆ†:ä½¿ç”¨å¾®å›¾å½¢å®ç°

> åŸæ–‡ï¼š<https://pyimagesearch.com/2022/12/26/automatic-differentiation-part-2-implementation-using-micrograd/>

* * *

## **ç›®å½•**

* * *

## [**è‡ªåŠ¨å¾®åˆ†ç¬¬äºŒéƒ¨åˆ†:ä½¿ç”¨å¾®å…‹å®ç°**](#TOC)

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åœ¨åä¸º`micrograd`çš„ Python åŒ…çš„å¸®åŠ©ä¸‹è¿›è¡Œè‡ªåŠ¨å¾®åˆ†ã€‚

æœ¬è¯¾æ˜¯å…³äº **Autodiff 101 çš„ä¸¤éƒ¨åˆ†ç³»åˆ—çš„æœ€åä¸€è¯¾â€”â€”ä»å¤´å¼€å§‹ç†è§£è‡ªåŠ¨åŒºåˆ†**:

1.  [*è‡ªåŠ¨å¾®åˆ†ç¬¬ä¸€éƒ¨åˆ†:ç†è§£æ•°å­¦*](https://pyimg.co/pyxml)
2.  [***è‡ªåŠ¨å¾®åˆ†ç¬¬äºŒéƒ¨åˆ†:ä½¿ç”¨å¾®å…‹å®ç°***](https://pyimg.co/ra6ow) **(ä»Šæ—¥æ•™ç¨‹)**

**è¦å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Python å®ç°è‡ªåŠ¨å¾®åˆ†ï¼Œ*ç»§ç»­é˜…è¯»å³å¯ã€‚***

* * *

## [**è‡ªåŠ¨å¾®åˆ†ç¬¬äºŒéƒ¨åˆ†:ä½¿ç”¨å¾®å…‹å®ç°**](#TOC)

* * *

### [**ç®€ä»‹**](#TOC)

* * *

#### [**ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ**](#TOC)

ç¥ç»ç½‘ç»œæ˜¯æˆ‘ä»¬å¤§è„‘çš„æ•°å­¦æŠ½è±¡(è‡³å°‘ï¼Œå®ƒæ˜¯è¿™æ ·å¼€å§‹çš„)ã€‚è¯¥ç³»ç»Ÿç”±è®¸å¤šå¯å­¦ä¹ çš„æ—‹é’®(æƒé‡å’Œåå·®)å’Œä¸€ä¸ªç®€å•çš„æ“ä½œ(ç‚¹ç§¯)ç»„æˆã€‚ç¥ç»ç½‘ç»œæ¥å—è¾“å…¥ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡æ—‹è½¬æ—‹é’®æ¥ä¼˜åŒ–è¯¥å‡½æ•°ã€‚è°ƒè°æ—‹é’®çš„æœ€ä½³æ–¹å¼æ˜¯ä½¿ç”¨ç›®æ ‡å‡½æ•°ç›¸å¯¹äºæ‰€æœ‰å•ä¸ªæ—‹é’®çš„æ¢¯åº¦ä½œä¸ºä¿¡å·ã€‚

å¦‚æœä½ åä¸‹æ¥è¯•ç€ç”¨æ‰‹ç®—ä¸€ä¸‹æ¢¯åº¦ï¼Œä¼šèŠ±å¾ˆé•¿æ—¶é—´ã€‚æ‰€ä»¥ï¼Œä¸ºäº†ç»•è¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†çš„æ¦‚å¿µã€‚

åœ¨[ä¹‹å‰çš„æ•™ç¨‹](https://pyimg.co/pyxml)ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†è‡ªåŠ¨å¾®åˆ†çš„æ•°å­¦ã€‚æœ¬æ•™ç¨‹å°†åº”ç”¨æ¦‚å¿µï¼Œä»å¤´å¼€å§‹ç†è§£è‡ªåŠ¨å¾®åˆ† Python åŒ…ã€‚

æˆ‘ä»¬ä»Šå¤©è¦è®²çš„è¿™ä¸ªåŒ…å«åš [`micrograd`](https://github.com/karpathy/micrograd) ã€‚è¿™æ˜¯ä¸€ä¸ªç”±å®‰å¾·çƒˆÂ·å¡å¸•è¥¿åˆ›å»ºçš„å¼€æº Python åŒ…ã€‚æˆ‘ä»¬å·²ç»å­¦ä¹ äº†[è§†é¢‘è®²åº§](https://youtu.be/VMj-3S1tku0)ï¼ŒAndrej ä»é›¶å¼€å§‹æ„å»ºè¿™ä¸ªåŒ…ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è§†é¢‘è®²åº§åˆ†è§£åˆ°ä¸€ä¸ªåšå®¢ä¸­ï¼Œå¹¶æ·»åŠ æˆ‘ä»¬çš„æƒ³æ³•æ¥ä¸°å¯Œå†…å®¹ã€‚

* * *

### [**åœ¨é…ç½®å¼€å‘ç¯å¢ƒæ—¶é‡åˆ°äº†é—®é¢˜ï¼Ÿ**](#TOC)

è¯´äº†è¿™ä¹ˆå¤šï¼Œä½ æ˜¯:

*   æ—¶é—´ç´§è¿«ï¼Ÿ
*   äº†è§£ä½ é›‡ä¸»çš„è¡Œæ”¿é”å®šç³»ç»Ÿï¼Ÿ
*   æƒ³è¦è·³è¿‡ä¸å‘½ä»¤è¡Œã€åŒ…ç®¡ç†å™¨å’Œè™šæ‹Ÿç¯å¢ƒæ–—äº‰çš„éº»çƒ¦å—ï¼Ÿ
*   **å‡†å¤‡å¥½åœ¨æ‚¨çš„ Windowsã€macOS æˆ– Linux ç³»ç»Ÿä¸Šè¿è¡Œä»£ç *****ï¼Ÿ***

 *é‚£ä»Šå¤©å°±åŠ å…¥ [PyImageSearch å¤§å­¦](https://pyimagesearch.com/pyimagesearch-university/)å§ï¼

**è·å¾—æœ¬æ•™ç¨‹çš„ Jupyter ç¬”è®°æœ¬å’Œå…¶ä»– PyImageSearch æŒ‡å—ï¼Œè¿™äº›æŒ‡å—æ˜¯** ***é¢„å…ˆé…ç½®çš„*** **ï¼Œå¯ä»¥åœ¨æ‚¨çš„ç½‘ç»œæµè§ˆå™¨ä¸­è¿è¡Œåœ¨ Google Colab çš„ç”Ÿæ€ç³»ç»Ÿä¸Šï¼**æ— éœ€å®‰è£…ã€‚

æœ€æ£’çš„æ˜¯ï¼Œè¿™äº› Jupyter ç¬”è®°æœ¬å¯ä»¥åœ¨ Windowsã€macOS å’Œ Linux ä¸Šè¿è¡Œï¼

* * *

### [**å…³äº`micrograd`**](#TOC)

`micrograd`æ˜¯ä¸€ä¸ª Python åŒ…ï¼Œæ—¨åœ¨ç†è§£åå‘ç´¯ç§¯(åå‘ä¼ æ’­)è¿‡ç¨‹å¦‚ä½•åœ¨ PyTorch æˆ– Jax ç­‰ç°ä»£æ·±åº¦å­¦ä¹ åŒ…ä¸­å·¥ä½œã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„è‡ªåŠ¨å¾®åˆ†åŒ…ï¼Œä»…é€‚ç”¨äº**æ ‡é‡**ã€‚

* * *

### [**å¯¼å…¥å’Œè®¾ç½®**](#TOC)

```py
import math
import random
from typing import List, Tuple, Union
from matplotlib import pyplot as plt
```

* * *

### [**`Value`ç­**](#TOC)

æˆ‘ä»¬ä»å®šä¹‰`Value`ç±»å¼€å§‹ã€‚ä¸ºäº†ä»¥åè¿›è¡Œè·Ÿè¸ªå’Œåå‘ä¼ æ’­ï¼Œå°†åŸå§‹æ ‡é‡å€¼åŒ…è£…åˆ°`Value`ç±»ä¸­å˜å¾—éå¸¸é‡è¦ã€‚

å½“åŒ…è£…åœ¨`Value`ç±»ä¸­æ—¶ï¼Œæ ‡é‡å€¼è¢«è®¤ä¸ºæ˜¯å›¾å½¢çš„**èŠ‚ç‚¹**ã€‚å½“æˆ‘ä»¬ä½¿ç”¨`Value` s å¹¶å»ºç«‹ä¸€ä¸ªç­‰å¼æ—¶ï¼Œè¿™ä¸ªç­‰å¼è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ª[æœ‰å‘æ— ç¯å›¾](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAG)ã€‚åœ¨*æ¼”ç®—*å’Œ*å›¾éå†*çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬è‡ªåŠ¨è®¡ç®—èŠ‚ç‚¹çš„æ¢¯åº¦(autodiff)å¹¶é€šè¿‡å®ƒä»¬åå‘ä¼ æ’­ã€‚

`Value`ç±»å…·æœ‰ä»¥ä¸‹å±æ€§:

*   `data`:éœ€è¦åŒ…è£…åœ¨`Value`ç±»ä¸­çš„åŸå§‹æµ®ç‚¹æ•°æ®ã€‚
*   `grad`:è¿™ä¸ªä¼šä¿å­˜èŠ‚ç‚¹çš„**å…¨å±€å¯¼æ•°**ã€‚å…¨å±€å¯¼æ•°æ˜¯æ ¹èŠ‚ç‚¹(æœ€ç»ˆèŠ‚ç‚¹)ç›¸å¯¹äºå½“å‰èŠ‚ç‚¹çš„åå¯¼æ•°ã€‚
*   è¿™æ˜¯ä¸€ä¸ªç§æœ‰æ–¹æ³•ï¼Œè®¡ç®—å½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹çš„å…¨å±€å¯¼æ•°ã€‚
*   `_prev`:å½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ã€‚

```py
class Value(object):
    """
    We need to wrap the raw data into a class that will store the
    metadata to help in automatic differentiation.

    Attributes:
        data (float): The data for the Value node.
        _children (Tuple): The children of the current node.
    """

    def __init__(self, data: float, _children: Tuple = ()):
        # The raw data for the Value node.
        self.data = data

        # The partial gradient of the last node with respect to this
        # node. This is also termed as the global gradient.
        # Gradient 0.0 means that there is no effect of the change
        # of the last node with respect to this node. On
        # initialization it is assumed that all the variables have no
        # effect on the entire architecture.
        self.grad = 0.0

        # The function that derives the gradient of the children nodes
        # of the current node. It is easier this way, because each node
        # is built from children nodes and an operation. Upon back-propagation
        # the current node can easily fill in the gradients of the children.
        # Note: The global gradient is the multiplication of the local gradient
        # and the flowing gradient from the parent.
        self._backward = lambda: None

        # Define the children of this node.
        self._prev = set(_children)

    def __repr__(self):
        # This is the string representation of the Value node.
        return f"Value(data={self.data}, grad={self.grad})"
```

```py
# Build a Value node
raw_data = 5.0
print(f"Raw Data(data={raw_data}, type={type(raw_data)}")
value_node = Value(data=raw_data)

# Calling the `__repr__` function here
print(value_node)
```

```py
>>> Raw Data(data=5.0, type=<class 'float'>
>>> Value(data=5.0, grad=0.0)
```

* * *

### [**åŠ æ³•**](#TOC)

ç°åœ¨æˆ‘ä»¬å·²ç»æ„å»ºäº†æˆ‘ä»¬çš„`Value`ç±»ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰åŸå§‹æ“ä½œå’Œå®ƒä»¬çš„`_backward`å‡½æ•°ã€‚è¿™å°†æœ‰åŠ©äºè·Ÿè¸ªæ¯ä¸ªèŠ‚ç‚¹çš„æ“ä½œï¼Œå¹¶é€šè¿‡ DAG è¡¨è¾¾å¼åå‘ä¼ æ’­æ¢¯åº¦ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¤„ç†**åŠ æ³•**æ“ä½œã€‚è¿™å°†æœ‰åŠ©äºä¸¤ä¸ªå€¼ç›¸åŠ ã€‚å½“æˆ‘ä»¬ä½¿ç”¨`+`æ“ä½œç¬¦æ—¶ï¼ŒPython ç±»æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•`__add__`è¢«è°ƒç”¨ï¼Œå¦‚å›¾**å›¾ 1** æ‰€ç¤ºã€‚

è¿™é‡Œæˆ‘ä»¬åˆ›å»ºäº†`custom_addition`å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°åæ¥è¢«åˆ†é…ç»™äº†`Value`ç±»çš„`__add__`æ–¹æ³•ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†è®©æˆ‘ä»¬æŠŠé‡ç‚¹æ”¾åœ¨åŠ æ³•æ–¹æ³•ä¸Šï¼Œèˆå¼ƒä¸€åˆ‡å¯¹åŠ æ³•è¿ç®—ä¸é‡è¦çš„ä¸œè¥¿ã€‚

åŠ æ³•è¿ç®—éå¸¸ç®€å•:

1.  å°†`self`å’Œ`other`èŠ‚ç‚¹ä½œä¸ºè°ƒç”¨çš„å‚æ•°ã€‚ç„¶åæˆ‘ä»¬å–ä»–ä»¬çš„`data`å¹¶åº”ç”¨åŠ æ³•ã€‚
2.  ç„¶åï¼Œç»“æœè¢«åŒ…è£…åœ¨`Value`ç±»ä¸­ã€‚
3.  èŠ‚ç‚¹`out`è¢«åˆå§‹åŒ–ï¼Œè¿™é‡Œæˆ‘ä»¬æåˆ°`self`å’Œ`other`æ˜¯å®ƒçš„å­©å­ã€‚

* * *

#### [**è®¡ç®—æ¢¯åº¦**](#TOC)

å¯¹äºæˆ‘ä»¬å®šä¹‰çš„æ¯ä¸ªåŸå§‹æ“ä½œï¼Œæˆ‘ä»¬éƒ½ä¼šæœ‰è¿™ä¸ªéƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†è®¡ç®—å­èŠ‚ç‚¹çš„å…¨å±€æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰`addition`æ“ä½œçš„å±€éƒ¨æ¢¯åº¦ã€‚

è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªèŠ‚ç‚¹![c](img/a0b01076dc0b1ee37de0d70233938435.png "c")

that is built by adding two children nodes ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"). Then, the partial derivatives of ![c](img/a0b01076dc0b1ee37de0d70233938435.png "c")are derived in **Figure 2**.

ç°åœ¨æƒ³æƒ³åå‘ä¼ æ’­ã€‚æŸå¤±(ç›®æ ‡)å‡½æ•°çš„åå¯¼æ•°![l](img/9519768569e43f86a89931b7a7264c02.png "l")

is already deduced for ![c](img/a0b01076dc0b1ee37de0d70233938435.png "c"). This means we have ![{(\partial{l})}/{(\partial{c})}](img/3bee520599a2fe013f4fae7ec2181a81.png "{(\partial{l})}/{(\partial{c})}"). This gradient needs to flow to the child nodes ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"), respectively.

åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")çš„å…¨å±€æ¢¯åº¦

and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"), as shown in **Figure 3**.

åŠ æ³•è¿ç®—å°±åƒä¸€ä¸ª**è·¯ç”±å™¨**ä½œç”¨äºæµå…¥çš„æ¸å˜ã€‚å®ƒå°†æ¸å˜è·¯ç”±åˆ°æ‰€æœ‰çš„å­èŠ‚ç‚¹ã€‚

â¤ ***æ³¨:*** åœ¨æˆ‘ä»¬å®šä¹‰çš„`_backward`å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ç”¨`+=`è¿ç®—ç´¯åŠ å­ä»£çš„æ¸å˜ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†ç»•è¿‡ä¸€ä¸ªç‹¬ç‰¹çš„æƒ…å†µã€‚å‡è®¾æˆ‘ä»¬æœ‰![c = a + a](img/4f6ea1bccca7d1500db95c6002ec4e36.png "c = a + a")

. Here we know that the expression can be simplified to ![c = 2a](img/570a09b67c5f388278dc1672657e4d28.png "c = 2a"), but our `_backward` for `__add__` does not know how to do this. The `__backward__` in `__add__` treats one ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")as `self` and the other ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")as `other`. If the gradients are not accumulated, we will see a discrepancy with the gradients.

```py
def custom_addition(self, other: Union["Value", float]) -> "Value":
    """
    The addition operation for the Value class.
    Args:
        other (Union["Value", float]): The other value to add to this one.
    Usage:
        >>> x = Value(2)
        >>> y = Value(3)
        >>> z = x + y
        >>> z.data
        5
    """
    # If the other value is not a Value, then we need to wrap it.
    other = other if isinstance(other, Value) else Value(other)

    # Create a new Value node that will be the output of the addition.
    out = Value(data=self.data + other.data, _children=(self, other))

    def _backward():
        # Local gradient:
        # x = a + b
        # dx/da = 1
        # dx/db = 1
        # Global gradient with chain rule:
        # dy/da = dy/dx . dx/da = dy/dx . 1
        # dy/db = dy/dx . dx/db = dy/dx . 1
        self.grad += out.grad * 1.0
        other.grad += out.grad * 1.0

    # Set the backward function on the output node.
    out._backward = _backward
    return out

def custom_reverse_addition(self, other):
    """
    Reverse addition operation for the Value class.
    Args:
        other (float): The other value to add to this one.
    Usage:
        >>> x = Value(2)
        >>> y = Value(3)
        >>> z = y + x
        >>> z.data
        5
    """
    # This is the same as adding. We can reuse the __add__ method.
    return self + other

Value.__add__ = custom_addition
Value.__radd__ = custom_reverse_addition
```

```py
# Build a and b
a = Value(data=5.0)
b = Value(data=6.0)

# Print the addition
print(f"{a} + {b} => {a+b}")
```

```py
>>> Value(data=5.0, grad=0.0) + Value(data=6.0, grad=0.0) => Value(data=11.0, grad=0.0)
```

```py
# Add a and b
c = a + b

# Assign a global gradient to c
c.grad = 11.0
print(f"c => {c}")

# Now apply `_backward` to c
c._backward()
print(f"a => {a}")
print(f"b => {b}")
```

```py
>>> c => Value(data=11.0, grad=11.0)
>>> a => Value(data=5.0, grad=11.0)
>>> b => Value(data=6.0, grad=11.0)
```

â¤ ***æ³¨:***![c](img/a0b01076dc0b1ee37de0d70233938435.png "c")çš„å…¨å±€æ¸å˜

is routed to ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b").

* * *

### [**ä¹˜æ³•**](#TOC)

åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¤„ç†**ä¹˜æ³•**è¿ç®—ã€‚å½“æˆ‘ä»¬ä½¿ç”¨`*`æ“ä½œç¬¦æ—¶ï¼ŒPython ç±»æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•`__mul__`è¢«è°ƒç”¨ï¼Œå¦‚å›¾**å›¾ 4** æ‰€ç¤ºã€‚

æˆ‘ä»¬å°†`self`å’Œ`other`èŠ‚ç‚¹ä½œä¸ºè°ƒç”¨çš„å‚æ•°ã€‚ç„¶åæˆ‘ä»¬å–ä»–ä»¬çš„`data`å¹¶åº”ç”¨ä¹˜æ³•ã€‚ç„¶åç»“æœè¢«åŒ…è£…åœ¨`Value`ç±»ä¸­ã€‚æœ€åï¼Œ`out`èŠ‚ç‚¹è¢«åˆå§‹åŒ–ï¼Œè¿™é‡Œæˆ‘ä»¬æåˆ°`self`å’Œ`other`æ˜¯å®ƒçš„å­èŠ‚ç‚¹ã€‚

* * *

#### [**è®¡ç®—æ¢¯åº¦**](#TOC)

è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªèŠ‚ç‚¹![c](img/a0b01076dc0b1ee37de0d70233938435.png "c")

that is built by multiplying two children nodes ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"). Then, the partial derivatives of ![c](img/a0b01076dc0b1ee37de0d70233938435.png "c")are shown in **Figure 5**.

ç°åœ¨æƒ³æƒ³åå‘ä¼ æ’­ã€‚æŸå¤±(ç›®æ ‡)å‡½æ•°çš„åå¯¼æ•°![l](img/9519768569e43f86a89931b7a7264c02.png "l")

is already deduced for ![c](img/a0b01076dc0b1ee37de0d70233938435.png "c"). This means we have ![{(\partial{l})}/{(\partial{c})}](img/3bee520599a2fe013f4fae7ec2181a81.png "{(\partial{l})}/{(\partial{c})}"). This gradient needs to flow to the children nodes ![a ](img/c4e62a1c11f14b613eb072299e413d2b.png "a ")and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"), respectively.

åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")çš„å…¨å±€æ¢¯åº¦

and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"), as shown in **Figure 6**.

```py
def custom_multiplication(self, other: Union["Value", float]) -> "Value":
    """
    The multiplication operation for the Value class.
    Args:
        other (float): The other value to multiply to this one.
    Usage:
        >>> x = Value(2)
        >>> y = Value(3)
        >>> z = x * y
        >>> z.data
        6
    """
    # If the other value is not a Value, then we need to wrap it.
    other = other if isinstance(other, Value) else Value(other)

    # Create a new Value node that will be the output of
    # the multiplication.
    out = Value(data=self.data * other.data, _children=(self, other))

    def _backward():
        # Local gradient:
        # x = a * b
        # dx/da = b
        # dx/db = a
        # Global gradient with chain rule:
        # dy/da = dy/dx . dx/da = dy/dx . b
        # dy/db = dy/dx . dx/db = dy/dx . a
        self.grad += out.grad * other.data
        other.grad += out.grad * self.data

    # Set the backward function on the output node.
    out._backward = _backward
    return out

def custom_reverse_multiplication(self, other):
    """
    Reverse multiplication operation for the Value class.
    Args:
        other (float): The other value to multiply to this one.
    Usage:
        >>> x = Value(2)
        >>> y = Value(3)
        >>> z = y * x
        >>> z.data
        6
    """
    # This is the same as multiplying. We can reuse the __mul__ method.
    return self * other

Value.__mul__ = custom_multiplication
Value.__rmul__ = custom_reverse_multiplication
```

```py
# Build a and b
a = Value(data=5.0)
b = Value(data=6.0)

# Print the multiplication
print(f"{a} * {b} => {a*b}")
```

```py
>>> Value(data=5.0, grad=0.0) * Value(data=6.0, grad=0.0) => Value(data=30.0, grad=0.0)
```

```py
# Multiply a and b
c = a * b

# Assign a global gradient to c
c.grad = 11.0
print(f"c => {c}")

# Now apply `_backward` to c
c._backward()
print(f"a => {a}")
print(f"b => {b}")
```

```py
>>> c => Value(data=30.0, grad=11.0)
>>> a => Value(data=5.0, grad=66.0)
>>> b => Value(data=6.0, grad=55.0)
```

* * *

### [**åŠ›é‡**](#TOC)

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¤„ç†**ç”µæº**çš„æ“ä½œã€‚Python ç±»æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•`__pow__`ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨`**`æ“ä½œç¬¦æ—¶ä¼šè°ƒç”¨è¿™ä¸ªæ–¹æ³•ï¼Œå¦‚å›¾**å›¾ 7** æ‰€ç¤ºã€‚

åœ¨è·å¾—ä½œä¸ºè°ƒç”¨å‚æ•°çš„`self`å’Œ`other`èŠ‚ç‚¹åï¼Œæˆ‘ä»¬è·å–å®ƒä»¬çš„`data`å¹¶åº”ç”¨å¹‚è¿ç®—ã€‚

* * *

#### [**è®¡ç®—æ¢¯åº¦**](#TOC)

è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªèŠ‚ç‚¹![c](img/a0b01076dc0b1ee37de0d70233938435.png "c")

that is built by multiplying two children nodes ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"). Then, the partial derivatives of ![c](img/a0b01076dc0b1ee37de0d70233938435.png "c")are derived in **Figure 8**.

ç°åœ¨æƒ³æƒ³åå‘ä¼ æ’­ã€‚æŸå¤±(ç›®æ ‡)å‡½æ•°çš„åå¯¼æ•°![l](img/9519768569e43f86a89931b7a7264c02.png "l")

is already deduced for ![c](img/a0b01076dc0b1ee37de0d70233938435.png "c"). This means we have ![{(\partial{l})}/{(\partial{c})}](img/3bee520599a2fe013f4fae7ec2181a81.png "{(\partial{l})}/{(\partial{c})}"). This gradient needs to flow to the child node ![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a").

åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°![a](img/64d3bff4db63ea2502cd130d8d9db723.png "a")çš„å…¨å±€æ¢¯åº¦

and ![b](img/d64e1379360a21ccd8dc595db9b1d8ad.png "b"), as shown in **Figure 9**.

```py
def custom_power(self, other):
    """
    The power operation for the Value class.
    Args:
        other (float): The other value to raise this one to.
    Usage:
        >>> x = Value(2)
        >>> z = x ** 2.0
        >>> z.data
        4
    """
    assert isinstance(
        other, (int, float)
    ), "only supporting int/float powers for now"

    # Create a new Value node that will be the output of the power.
    out = Value(data=self.data ** other, _children=(self,))

    def _backward():
        # Local gradient:
        # x = a ** b
        # dx/da = b * a ** (b - 1)
        # Global gradient:
        # dy/da = dy/dx . dx/da = dy/dx . b * a ** (b - 1)
        self.grad += out.grad * (other * self.data ** (other - 1))

    # Set the backward function on the output node.
    out._backward = _backward
    return out

Value.__pow__ = custom_power
```

```py
# Build a
a = Value(data=5.0)
# For power operation we will use
# the raw data and not wrap it into
# a node. This is done for simplicity.
b = 2.0

# Print the power operation
print(f"{a} ** {b} => {a**b}")
```

```py
>>> Value(data=5.0, grad=0.0) ** 2.0 => Value(data=25.0, grad=0.0)
```

```py
# Raise a to the power of b
c = a ** b

# Assign a global gradient to c
c.grad = 11.0
print(f"c => {c}")

# Now apply `_backward` to c
c._backward()
print(f"a => {a}")
print(f"b => {b}")
```

```py
>>> c => Value(data=25.0, grad=11.0)
>>> a => Value(data=5.0, grad=110.0)
>>> b => 2.0
```

* * *

### [**å¦å®š**](#TOC)

å¯¹äº**å¦å®š**æ“ä½œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„`__mul__`æ“ä½œã€‚æ­¤å¤–ï¼ŒPython ç±»æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•`__neg__`ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ä¸€å…ƒ`-`æ“ä½œç¬¦æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•è¢«è°ƒç”¨ï¼Œå¦‚å›¾**å›¾ 10** æ‰€ç¤ºã€‚

è¿™æ„å‘³ç€å¦å®šçš„`_backward`å°†è¢«å¤„ç†ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ˜ç¡®åœ°å®šä¹‰å®ƒã€‚

```py
def custom_negation(self):
    """
    Negation operation for the Value class.
    Usage:
        >>> x = Value(2)
        >>> z = -x
        >>> z.data
        -2
    """
    # This is the same as multiplying by -1\. We can reuse the
    # __mul__ method.
    return self * -1

Value.__neg__ = custom_negation
```

```py
# Build `a`
a = Value(data=5.0)

# Print the negation
print(f"Negation of {a} => {(-a)}")
```

```py
>>> Negation of Value(data=5.0, grad=0.0) => Value(data=-5.0, grad=0.0)
```

```py
# Negate a
c = -a

# Assign a global gradient to c
c.grad = 11.0
print(f"c => {c}")

# Now apply `_backward` to c
c._backward()
print(f"a => {a}")
```

```py
>>> c => Value(data=-5.0, grad=11.0)
>>> a => Value(data=5.0, grad=-11.0)
```

* * *

### [**å‡æ³•**](#TOC)

**å‡æ³•**æ“ä½œå¯ä»¥ç”¨`__add__`å’Œ`__neg__`æ¥å¤„ç†ã€‚æ­¤å¤–ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨`-`æ“ä½œç¬¦æ—¶ï¼ŒPython ç±»æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•`__sub__`è¢«è°ƒç”¨ï¼Œå¦‚å›¾**å›¾ 11** æ‰€ç¤ºã€‚

è¿™å°†å¸®åŠ©æˆ‘ä»¬å°†`_backward`å‡æ³•è¿ç®—å§”æ‰˜ç»™åŠ æ³•å’Œå¦å®šè¿ç®—ã€‚

```py
def custom_subtraction(self, other):
    """
    Subtraction operation for the Value class.
    Args:
        other (float): The other value to subtract to this one.
    Usage:
        >>> x = Value(2)
        >>> y = Value(3)
        >>> z = x - y
        >>> z.data
        -1
    """
    # This is the same as adding the negative of the other value.
    # We can reuse the __add__ and the __neg__ methods.
    return self + (-other)

def custom_reverse_subtraction(self, other):
    """
    Reverse subtraction operation for the Value class.
    Args:
        other (float): The other value to subtract to this one.
    Usage:
        >>> x = Value(2)
        >>> y = Value(3)
        >>> z = y - x
        >>> z.data
        1
    """
    # This is the same as subtracting. We can reuse the __sub__ method.
    return other + (-self)

Value.__sub__ = custom_subtraction
Value.__rsub__ = custom_reverse_subtraction
```

```py
# Build a and b
a = Value(data=5.0)
b = Value(data=4.0)

# Print the negation
print(f"{a} - {b} => {(a-b)}")
```

```py
>>> Value(data=5.0, grad=0.0) - Value(data=4.0, grad=0.0) => Value(data=1.0, grad=0.0)
```

```py
# Subtract b from a
c = a - b

# Assign a global gradient to c
c.grad = 11.0
print(f"c => {c}")

# Now apply `_backward` to c
c._backward()
print(f"a => {a}")
print(f"b => {b}")
```

```py
>>> c => Value(data=1.0, grad=11.0)
>>> a => Value(data=5.0, grad=11.0)
>>> b => Value(data=4.0, grad=0.0)
```

â¤ ***æ³¨:*** æ¸å˜å¹¶æ²¡æœ‰åƒçº¸ä¸Šæƒ³è±¡çš„é‚£æ ·æµåŠ¨ã€‚ä¸ºä»€ä¹ˆï¼Ÿä½ èƒ½æƒ³å‡ºè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆå—ï¼Ÿ

â¤ ***æç¤º:*** å‡æ³•è¿ç®—ç”±ä¸€ä¸ªä»¥ä¸Šçš„åŸå§‹è¿ç®—ç»„æˆ:å¦å®šå’ŒåŠ æ³•ã€‚

æˆ‘ä»¬å°†åœ¨æ•™ç¨‹çš„åé¢è®¨è®ºè¿™ä¸€ç‚¹ã€‚

* * *

### [**å¸ˆ**](#TOC)

**åˆ†å‰²**æ“ä½œå¯ä»¥ç”¨`__mul__`å’Œ`__pow__`æ¥å¤„ç†ã€‚æ­¤å¤–ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨`/`æ“ä½œç¬¦æ—¶ï¼ŒPython ç±»æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•`__div__`è¢«è°ƒç”¨ï¼Œå¦‚å›¾**å›¾ 12** æ‰€ç¤ºã€‚

è¿™å°†å¸®åŠ©æˆ‘ä»¬å°†`_backward`é™¤æ³•è¿ç®—å§”æ‰˜ç»™å¹‚è¿ç®—ã€‚

```py
def custom_division(self, other):
    """
    Division operation for the Value class.
    Args:
        other (float): The other value to divide to this one.
    Usage:
        >>> x = Value(10)
        >>> y = Value(5)
        >>> z = x / y
        >>> z.data
        2
    """
    # Use the __pow__ method to implement division.
    return self * other ** -1

def custom_reverse_division(self, other):
    """
    Reverse division operation for the Value class.
    Args:
        other (float): The other value to divide to this one.
    Usage:
        >>> x = Value(10)
        >>> y = Value(5)
        >>> z = y / x
        >>> z.data
        0.5
    """
    # Use the __pow__ method to implement division.
    return other * self ** -1

Value.__truediv__ = custom_division
Value.__rtruediv__ = custom_reverse_division
```

```py
# Build a and b
a = Value(data=6.0)
b = Value(data=3.0)

# Print the negation
print(f"{a} / {b} => {(a/b)}")
```

```py
>>> Value(data=6.0, grad=0.0) / Value(data=3.0, grad=0.0) => Value(data=2.0, grad=0.0)
```

```py
# Divide a with b
c = a / b

# Assign a global gradient to c
c.grad = 11.0
print(f"c => {c}")

# Now apply `_backward` to c
c._backward()
print(f"a => {a}")
print(f"b => {b}")
```

```py
>>> c => Value(data=2.0, grad=11.0)
>>> a => Value(data=6.0, grad=3.6666666666666665)
>>> b => Value(data=3.0, grad=0.0)
```

â¤ï¼Œåœ¨é™¤æ³•ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å’Œå‡æ³•ä¸€æ ·çš„æ¢¯åº¦æµé—®é¢˜ã€‚ä½ å·²ç»è§£å†³é—®é¢˜äº†å—ï¼ŸğŸ‘€

* * *

### [**RectivedLlinearUnit**](#TOC)

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»éçº¿æ€§ã€‚ReLU æ˜¯**ä¸æ˜¯**ä¸€ä¸ªåŸå‡½æ•°ï¼›æˆ‘ä»¬éœ€è¦ä¸ºå®ƒæ„å»ºå‡½æ•°å’Œ`_backward`å‡½æ•°ã€‚

```py
def relu(self):
    """
    The ReLU activation function.
    Usage:
        >>> x = Value(-2)
        >>> y = x.relu()
        >>> y.data
        0
    """
    out = Value(data=0 if self.data < 0 else self.data, _children=(self,))

    def _backward():
        # Local gradient:
        # x = relu(a)
        # dx/da = 0 if a < 0 else 1
        # Global gradient:
        # dy/da = dy/dx . dx/da = dy/dx . (0 if a < 0 else 1)
        self.grad += out.grad * (out.data > 0)

    # Set the backward function on the output node.
    out._backward = _backward
    return out

Value.relu = relu
```

```py
# Build a
a = Value(data=6.0)

# Print a and the negation
print(f"ReLU ({a}) => {(a.relu())}")
print(f"ReLU (-{a}) => {((-a).relu())}")
```

```py
>>> ReLU (Value(data=6.0, grad=0.0)) => Value(data=6.0, grad=0.0)
>>> ReLU (-Value(data=6.0, grad=0.0)) => Value(data=0, grad=0.0)
```

```py
# Build a and b
a = Value(3.0)
b = Value(-3.0)

# Apply relu on both the nodes
relu_a = a.relu()
relu_b = b.relu()

# Assign a global gradients
relu_a.grad = 11.0
relu_b.grad = 11.0

# Now apply `_backward`
relu_a._backward()
print(f"a => {a}")
relu_b._backward()
print(f"b => {b}")
```

```py
>>> a => Value(data=3.0, grad=11.0)
>>> b => Value(data=-3.0, grad=0.0)
```

* * *

### [**å…¨çƒè½å**](#TOC)

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»è®¾è®¡äº†åŸè¯­å’ŒéåŸè¯­(ReLU)å‡½æ•°åŠå…¶å„è‡ªçš„`_backward`æ–¹æ³•ã€‚æ¯ä¸ªå›¾å…ƒåªèƒ½å°†æµåŠ¨æ¸å˜æ”¯æŒåˆ°å…¶å­å›¾å…ƒã€‚

æˆ‘ä»¬ç°åœ¨å¿…é¡»è®¾è®¡ä¸€ç§æ–¹æ³•ï¼Œåœ¨ DAG(æ„å»ºçš„æ–¹ç¨‹)ä¸­è¿­ä»£æ‰€æœ‰è¿™æ ·çš„åŸå§‹æ–¹æ³•ï¼Œå¹¶åœ¨æ•´ä¸ªè¡¨è¾¾å¼ä¸­åå‘ä¼ æ’­æ¢¯åº¦ã€‚

ä¸ºæ­¤ï¼Œ`Value`è°ƒç”¨éœ€è¦ä¸€ä¸ªå…¨å±€`backward`æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ DAG çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ä¸Šåº”ç”¨`backward`å‡½æ•°ã€‚è¯¥å‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œ:

*   æŒ‰æ‹“æ‰‘é¡ºåºå¯¹ DAG è¿›è¡Œæ’åº
*   å°†æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„`grad`è®¾ç½®ä¸º 1.0
*   éå†æ‹“æ‰‘æ’åºçš„å›¾ï¼Œå¹¶åº”ç”¨æ¯ä¸ªåŸè¯­çš„`_backward`æ–¹æ³•ã€‚

```py
def backward(self):
    """
    The backward pass of the backward propagation algorithm.
    Usage:
        >>> x = Value(2)
        >>> y = Value(3)
        >>> z = x * y
        >>> z.backward()
        >>> x.grad
        3
        >>> y.grad
        2
    """
    # Build an empty list which will hold the
    # topologically sorted graph
    topo = []

    # Build a set of all the visited nodes
    visited = set()

    # A closure to help build the topologically sorted graph
    def build_topo(node: "Value"):
        if node not in visited:
            # If node is not visited add the node to the
            # visited set.
            visited.add(node)

            # Iterate over the children of the node that
            # is being visited
            for child in node._prev:
                # Apply recursion to build the topologically sorted
                # graph of the children
                build_topo(child)

            # Only append node to the topologically sorted list
            # if all its children are visited.
            topo.append(node)

    # Call the `build_topo` method on self
    build_topo(self)

    # Go one node at a time and apply the chain rule
    # to get its gradient
    self.grad = 1.0
    for node in reversed(topo):
        node._backward()

Value.backward = backward
```

```py
# Now create an expression that uses a lot of
# primitive operations
a = Value(2.0)
b = Value(3.0)
c = a+b
d = 4.0
e = c**d
f = Value(6.0)
g = e/f

print(â€œBEFORE backwardâ€)
for element in [a, b, c, d, e, f, g]:
    print(element)

# Backward on the final node will backprop
# the gradients through the entire DAG
g.backward()

print(â€œAFTER backwardâ€)
for element in [a, b, c, d, e, f, g]:
    print(element)
```

```py
>>> BEFORE backward
>>> Value(data=2.0, grad=0.0)
>>> Value(data=3.0, grad=0.0)
>>> Value(data=5.0, grad=0.0)
>>> 4.0
>>> Value(data=625.0, grad=0.0)
>>> Value(data=6.0, grad=0.0)
>>> Value(data=104.16666666666666, grad=0.0)

>>> AFTER backward
>>> Value(data=2.0, grad=83.33333333333333)
>>> Value(data=3.0, grad=83.33333333333333)
>>> Value(data=5.0, grad=83.33333333333333)
>>> 4.0
>>> Value(data=625.0, grad=0.16666666666666666)
>>> Value(data=6.0, grad=-17.36111111111111)
>>> Value(data=104.16666666666666, grad=1.0)
```

è¿˜è®°å¾—æˆ‘ä»¬å…³äº`__sub__`å’Œ`__div__`çš„é—®é¢˜å—ï¼Ÿæ¢¯åº¦å¹¶ä¸æŒ‰ç…§å¾®ç§¯åˆ†çš„è§„åˆ™åå‘ä¼ æ’­ã€‚å®ç°`_backward`åŠŸèƒ½æ²¡æœ‰é”™ã€‚

ä½†æ˜¯ï¼Œè¿™ä¸¤ä¸ªæ“ä½œ(`__sub__`å’Œ`__div__`)æ˜¯ç”¨ä¸æ­¢ä¸€ä¸ªåŸå§‹æ“ä½œ(`__neg__`å’Œ`__add__`ç”¨äº`__sub__`)æ„å»ºçš„ï¼›`__mul__`å’Œ`__pow__`ä¸º`__div__`ã€‚

è¿™ä¼šåˆ›å»ºä¸€ä¸ªä¸­é—´èŠ‚ç‚¹ï¼Œé˜»æ­¢æ¸å˜æ­£ç¡®åœ°æµå‘å­èŠ‚ç‚¹(è®°ä½ï¼Œ`_backward`ä¸åº”è¯¥é€šè¿‡æ•´ä¸ª DAG åå‘ä¼ æ’­æ¸å˜)ã€‚

```py
# Solve the problem with subtraction
a = Value(data=6.0)
b = Value(data=3.0)

c = a - b
c.backward()
print(f"c => {c}")
print(f"a => {a}")
print(f"b => {b}")
```

```py
c => Value(data=3.0, grad=1.0)
a => Value(data=6.0, grad=1.0)
b => Value(data=3.0, grad=-1.0)
```

```py
# Solve the problem with division
a = Value(data=6.0)
b = Value(data=3.0)

c = a / b
c.backward()
print(f"c => {c}")
print(f"a => {a}")
print(f"b => {b}")
```

```py
>>> c => Value(data=2.0, grad=1.0)
>>> a => Value(data=6.0, grad=0.3333333333333333)
>>> b => Value(data=3.0, grad=-0.6666666666666666)
```

* * *

### [**ç”¨`micrograd`**](#TOC) æ„å»ºå¤šå±‚æ„ŸçŸ¥å™¨

å¦‚æœæˆ‘ä»¬åªæ˜¯æ„å»º`Value`ç±»ï¼Œè€Œä¸æ˜¯ç”¨å®ƒæ¥æ„å»ºç¥ç»ç½‘ç»œï¼Œé‚£æœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿ

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªéå¸¸ç®€å•çš„ç¥ç»ç½‘ç»œ(å¤šå±‚æ„ŸçŸ¥å™¨),å¹¶ä½¿ç”¨å®ƒæ¥å»ºæ¨¡ä¸€ä¸ªç®€å•çš„æ•°æ®é›†ã€‚

* * *

#### [**æ¨¡å—**](#TOC)

è¿™æ˜¯çˆ¶ç±»ã€‚`Module`ç±»æœ‰ä¸¤ä¸ªæ–¹æ³•:

*   `zero_grad`:ç”¨äºå°†å‚æ•°çš„æ‰€æœ‰æ¢¯åº¦å½’é›¶ã€‚
*   `parameters`:è¯¥åŠŸèƒ½è¢«æ„å»ºä¸ºå¯è¢«è¦†ç›–ã€‚è¿™å°†æœ€ç»ˆä¸ºæˆ‘ä»¬è·å¾—**ç¥ç»å…ƒ**ã€**å±‚**å’Œ **mlp** çš„å‚æ•°ã€‚

```py
class Module(object):
    """
    The parent class for all neural network modules.
    """

    def zero_grad(self):
        # Zero out the gradients of all parameters.
        for p in self.parameters():
            p.grad = 0

    def parameters(self):
        # Initialize a parameters function that all the children will
        # override and return a list of parameters.
        return []
```

* * *

#### [**ç¥ç»å…ƒ**](#TOC)

è¿™æ˜¯æˆ‘ä»¬ç¥ç»ç½‘ç»œçš„å•å…ƒï¼Œæ•´ä¸ªç»“æ„å°±æ˜¯å»ºç«‹åœ¨è¿™ä¸ªå•å…ƒä¸Šçš„ã€‚å®ƒæœ‰ä¸€ä¸ªæƒé‡åˆ—è¡¨å’Œä¸€ä¸ªåå¥½ã€‚ç¥ç»å…ƒçš„åŠŸèƒ½å¦‚å›¾**å›¾ 13** æ‰€ç¤ºã€‚

```py
class Neuron(Module):
    """
    A single neuron.
    Parameters:
        number_inputs (int): number of inputs
        is_nonlinear (bool): whether to apply ReLU nonlinearity
        name (int): the index of neuron
    """

    def __init__(self, number_inputs: int, name, is_nonlinear: bool = True):
        # Create weights for the neuron. The weights are initialized
        # from a random uniform distribution.
        self.weights = [Value(data=random.uniform(-1, 1)) for _ in range(number_inputs)]

        # Create bias for the neuron.
        self.bias = Value(data=0.0)
        self.is_nonlinear = is_nonlinear

        self.name = name

    def __call__(self, x: List["Value"]) -> "Value":
        # Compute the dot product of the input and the weights. Add the
        # bias to the dot product.
        act = sum(
            ((wi * xi) for wi, xi in zip(self.weights, x)),
            self.bias
        )

        # If activation is mentioned, apply ReLU to it.
        return act.relu() if self.is_nonlinear else act

    def parameters(self):
        # Get the parameters of the neuron. The parameters of a neuron
        # is its weights and bias.
        return self.weights + [self.bias]

    def __repr__(self):
        # Print a better representation of the neuron.
        return f"Neuron {self.name}(Number={len(self.weights)}, Non-Linearity={'ReLU' if self.is_nonlinear else 'None'})"
```

```py
x = [2.0, 3.0]
neuron = Neuron(number_inputs=2, name=1)
print(neuron)
out = neuron(x)
print(f"Output => {out}")
```

```py
>>> Neuron 1(Number=2, Non-Linearity=ReLU)
>>> Output => Value(data=2.3063230206881347, grad=0.0)
```

* * *

#### [**å±‚**](#TOC)

ä¸€å±‚ç”±è®¸å¤š`Neuron`æ„æˆã€‚

```py
class Layer(Module):
    """
    A layer of neurons.
    Parameters:
        number_inputs (int): number of inputs
        number_outputs (int): number of outputs
        name (int): index of the layer
    """

    def __init__(self, number_inputs: int, number_outputs: int, name: int, **kwargs):
        # A layer is a list of neurons.
        self.neurons = [
            Neuron(number_inputs=number_inputs, name=idx, **kwargs) for idx in range(number_outputs)
        ]
        self.name = name
        self.number_outputs = number_outputs

    def __call__(self, x: List["Value"]) -> Union[List["Value"], "Value"]:
        # Iterate over all the neurons and compute the output of each.
        out = [n(x) for n in self.neurons]
        return out if self.number_outputs != 1 else out[0]

    def parameters(self):
        # The parameters of a layer is the parameters of all the neurons.
        return [p for n in self.neurons for p in n.parameters()]

    def __repr__(self):
        # Print a better representation of the layer.
        layer_str = "\n".join(f'    - {str(n)}' for n in self.neurons)
        return f"Layer {self.name} \n{layer_str}\n"
```

```py
x = [2.0, 3.0]
layer = Layer(number_inputs=2, number_outputs=3, name=1)
print(layer)
out = layer(x)
print(f"Output => {out}")
```

```py
>>> Layer 1 
>>>     - Neuron 0(Number=2, Non-Linearity=ReLU)
>>>     - Neuron 1(Number=2, Non-Linearity=ReLU)
>>>     - Neuron 2(Number=2, Non-Linearity=ReLU)

>>> Output => [Value(data=0, grad=0.0), Value(data=1.1705131190055296, grad=0.0), Value(data=3.0608608028649344, grad=0.0)]
```

```py
x = [2.0, 3.0]
layer = Layer(number_inputs=2, number_outputs=1, name=1)
print(layer)
out = layer(x)
print(f"Output => {out}")
```

```py
>>> Layer 1 
>>>     - Neuron 0(Number=2, Non-Linearity=ReLU)

>>> Output => Value(data=2.3123867684232247, grad=0.0)
```

* * *

#### [**å¤šå±‚æ„ŸçŸ¥å™¨**](#TOC)

å¤šå±‚æ„ŸçŸ¥å™¨(`MLP`)æ˜¯ç”±è®¸å¤š`Layer`ç»„æˆçš„ã€‚

```py
class MLP(Module):
    """
    The Multi-Layer Perceptron (MLP) class.
    Parameters:
        number_inputs (int): number of inputs.
        list_number_outputs (List[int]): number of outputs in each layer.
    """

    def __init__(self, number_inputs: int, list_number_outputs: List[int]):
        # Get the number of inputs and all the number of outputs in
        # a single list.
        total_size = [number_inputs] + list_number_outputs

        # Build layers by connecting each layer to the previous one.
        self.layers = [
            # Do not use non linearity in the last layer.
            Layer(
                number_inputs=total_size[i],
                number_outputs=total_size[i + 1],
                name=i,
                is_nonlinear=i != len(list_number_outputs) - 1
            )
            for i in range(len(list_number_outputs))
        ]

    def __call__(self, x: List["Value"]) -> List["Value"]:
        # Iterate over the layers and compute the output of
        # each sequentially.
        for layer in self.layers:
            x = layer(x)
        return x

    def parameters(self):
        # Get the parameters of the MLP
        return [p for layer in self.layers for p in layer.parameters()]

    def __repr__(self):
        # Print a better representation of the MLP.
        mlp_str = "\n".join(f'  - {str(layer)}' for layer in self.layers)
        return f"MLP of \n{mlp_str}"
```

```py
x = [2.0, 3.0]
mlp = MLP(number_inputs=2, list_number_outputs=[3, 3, 1])
print(mlp)
out = mlp(x)
print(f"Output => {out}")
```

```py
>>> MLP of 
>>>   - Layer 0 
>>>     - Neuron 0(Number=2, Non-Linearity=ReLU)
>>>     - Neuron 1(Number=2, Non-Linearity=ReLU)
>>>     - Neuron 2(Number=2, Non-Linearity=ReLU)

>>>   - Layer 1 
>>>     - Neuron 0(Number=3, Non-Linearity=ReLU)
>>>     - Neuron 1(Number=3, Non-Linearity=ReLU)
>>>     - Neuron 2(Number=3, Non-Linearity=ReLU)

>>>   - Layer 2 
>>>     - Neuron 0(Number=3, Non-Linearity=None)

>>> Output => Value(data=-0.3211612402687316, grad=0.0)
```

* * *

#### [**è®­ç»ƒ MLP**](#TOC)

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå°å‹æ•°æ®é›†ï¼Œå¹¶å°è¯•äº†è§£å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ MLP å¯¹æ•°æ®é›†è¿›è¡Œå»ºæ¨¡ã€‚

```py
# Build a dataset
xs = [
    [0.5, 0.5, 0.70],
    [0.4, -0.1, 0.5],
    [-0.2, -0.75, 1.0],
]
ys = [0.0, 1.0, 0.0]
```

```py
# Build an MLP
mlp = MLP(number_inputs=3, list_number_outputs=[3, 3, 1])
```

åœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸‰ä¸ªå‡½æ•°:

*   `forward`:å‰è¿›åŠŸèƒ½é‡‡ç”¨`mlp`å’Œè¾“å…¥ã€‚è¾“å…¥é€šè¿‡`mlp`è½¬å‘ï¼Œæˆ‘ä»¬ä»`mlp`è·å¾—é¢„æµ‹ã€‚
*   æˆ‘ä»¬æœ‰äº‹å®å’Œé¢„æµ‹ã€‚è¯¥å‡½æ•°è®¡ç®—ä¸¤è€…ä¹‹é—´çš„æŸè€—ã€‚æˆ‘ä»¬å°†ä¼˜åŒ–æˆ‘ä»¬çš„`mlp`ï¼Œä½¿æŸå¤±ä¸ºé›¶ã€‚
*   `update_mlp`:åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ç”¨æ¢¯åº¦ä¿¡æ¯æ›´æ–°`mlp`çš„å‚æ•°(æƒé‡å’Œåå·®)ã€‚

```py
def forward(mlp: "MLP", xs: List[List[float]]) -> List["Value"]:
    # Get the predictions upon forwarding the input data through
    # the mlp
    ypred = [mlp(x) for x in xs]
    return ypred
```

```py
def compute_loss(ys: List[int], ypred: List["Value"]) -> "Value":
    # Obtain the L2 distance of the prediction and ground truths
    loss = sum(
        [(ygt - yout)**2 for ygt, yout in zip(ys, ypred)]
    )
    return loss
```

```py
def update_mlp(mlp: "MLP"):
    # Iterate over all the layers of the MLP
    for layer in mlp.layers:
        # Iterate over all the neurons of each layer
        for neuron in layer.neurons:
            # Iterate over all the weights of each neuron
            for weight in neuron.weights:
                # Update the data of the weight with the 
                # gradient information.
                weight.data -= (1e-2 * weight.grad)
            # Update the data of the bias with the 
            # gradient information.
            neuron.bias.data -= (1e-2 * neuron.bias.grad)
```

```py
# Define the epochs for which we want to run the training process.
epochs = 50

# Define a loss list to help log the loss.
loss_list = []

# Iterate each epoch and train the model.
for idx in range(epochs):
    # Step 1: Forward the inputs to the mlp and get the predictions
    ypred = forward(mlp, xs)
    # Step 2: Compute Loss between the predictions and the ground truths
    loss = compute_loss(ys, ypred)
    # Step 3: Ground the gradients. These accumulate which is not desired.
    mlp.zero_grad()
    # Step 4: Backpropagate the gradients through the entire architecture
    loss.backward()
    # Step 5: Update the mlp
    update_mlp(mlp)
    # Step 6: Log the loss
    loss_list.append(loss.data)
    print(f"Epoch {idx}: Loss {loss.data: 0.2f}")
```

```py
Epoch 0: Loss  0.95
Epoch 1: Loss  0.89
Epoch 2: Loss  0.81
Epoch 3: Loss  0.74
Epoch 4: Loss  0.68
Epoch 5: Loss  0.63
Epoch 6: Loss  0.59
.
.
Epoch 47: Loss  0.24
Epoch 48: Loss  0.23
Epoch 49: Loss  0.22
```

```py
# Plot the loss
plt.plot(loss_list)
plt.grid()
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.show()
```

æŸå¤±å›¾å¦‚**å›¾ 14** æ‰€ç¤ºã€‚

```py
# Inference
pred = mlp(xs[0])
ygt = ys[0]

print(f"Prediction => {pred.data: 0.2f}")
print(f"Ground Truth => {ygt: 0.2f}")
```

```py
>>> Prediction =>  0.14
>>> Ground Truth =>  0.00
```

* * *

* * *

## [**æ±‡æ€»**](#TOC)

æˆ‘ä»¬å†™è¿™ç¯‡åšå®¢çš„ä¸»è¦ç›®çš„æ˜¯çœ‹çœ‹è‡ªåŠ¨æŒ–æ˜è¿‡ç¨‹çš„å†…å¹•ã€‚åœ¨ Andrej çš„`micrograd`åº“çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬ç°åœ¨çŸ¥é“äº†å¦‚ä½•æ„å»ºä¸€ä¸ªéå¸¸å°ä½†æœ‰æ•ˆçš„ autodiff åŒ…ã€‚

æˆ‘ä»¬å¸Œæœ› **autodiff** ã€**ã€åå‘ä¼ æ’­ã€**å’ŒåŸºæœ¬ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ ¸å¿ƒæ¦‚å¿µç°åœ¨å¯¹ä½ å·²ç»å¾ˆæ¸…æ¥šäº†ã€‚

è®©æˆ‘ä»¬çŸ¥é“ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ã€‚

**æ¨ç‰¹:** [@PyImageSearch](https://twitter.com/pyimagesearch)

* * *

### [**å¼•ç”¨ä¿¡æ¯**](#TOC)

A. R. Gosthipaty å’Œ R. Rahaã€‚â€œè‡ªåŠ¨å¾®åˆ†ç¬¬äºŒéƒ¨åˆ†:ä½¿ç”¨å¾®å›¾å®ç°â€ï¼Œ *PyImageSearch* ï¼ŒP. Chughï¼ŒS. Huotï¼ŒK. Kidriavstevaï¼ŒA. Thankiï¼Œ2022ï¼Œ[https://pyimg.co/ra6ow](https://pyimg.co/ra6ow)

```py
@incollection{ARG-RR_2022_autodiff2,
  author = {Aritra Roy Gosthipaty and Ritwik Raha},
  title = {Automatic Differentiation Part 2: Implementation Using Micrograd},
  booktitle = {PyImageSearch},
  editor = {Puneet Chugh and Susan Huot and Kseniia Kidriavsteva and Abhishek Thanki},
  year = {2022},
  note = {https://pyimg.co/ra6ow},
}
```

* * *

* * *

**è¦ä¸‹è½½è¿™ç¯‡æ–‡ç« çš„æºä»£ç (å¹¶åœ¨æœªæ¥æ•™ç¨‹åœ¨ PyImageSearch ä¸Šå‘å¸ƒæ—¶å¾—åˆ°é€šçŸ¥)ï¼Œ*åªéœ€åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­è¾“å…¥æ‚¨çš„ç”µå­é‚®ä»¶åœ°å€ï¼****